{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to PyTorch\n",
    "\n",
    "PyTorch is a Python package for performing tensor computation, automatic differentiation, and dynamically defining neural networks. It makes it particularly easy to accelerate model training with a GPU. In recent years it has gained a large following in the NLP community.\n",
    "\n",
    "\n",
    "## Installing PyTorch\n",
    "\n",
    "Instructions for installing PyTorch can be found on the home-page of their website: <http://pytorch.org/>. The PyTorch developers recommended you use the `conda` package manager to install the library (in my experience `pip` works fine as well).\n",
    "\n",
    "One thing to be aware of is that the package name will be different depending on whether or not you intend on using a GPU. If you do plan on using a GPU, then you will need to install CUDA and CUDNN before installing PyTorch. Detailed instructions can be found at NVIDIA's website: <https://docs.nvidia.com/cuda/>. The following versions of CUDA are supported: 7.5, 8, and 9.\n",
    "\n",
    "\n",
    "## PyTorch Basics\n",
    "\n",
    "The PyTorch API is designed to very closely resemble NumPy. The central object for performing computation is the `Tensor`, which is PyTorch's version of NumPy's `array`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.68156159e+154, 2.68156159e+154],\n",
       "       [2.23901381e-314, 2.23902153e-314],\n",
       "       [2.23862805e-314, 0.00000000e+000]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a 3 x 2 array\n",
    "np.ndarray((3, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.6589e+23, 1.0187e-11],\n",
       "        [1.0514e-05, 1.1039e-05],\n",
       "        [1.0978e-05, 4.1291e-05]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a 3 x 2 Tensor\n",
    "torch.Tensor(3, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the basic arithmetic operations are supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a + b: tensor([4., 6.])\n",
      "a - b: tensor([-2., -2.])\n",
      "a * b: tensor([3., 8.])\n",
      "a / b: tensor([0.3333, 0.5000])\n"
     ]
    }
   ],
   "source": [
    "a = torch.Tensor([1,2])\n",
    "b = torch.Tensor([3,4])\n",
    "print('a + b:', a + b)\n",
    "print('a - b:', a - b)\n",
    "print('a * b:', a * b)\n",
    "print('a / b:', a / b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexing/slicing also behaves the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: tensor([[9, 5, 7, 4],\n",
      "        [9, 7, 8, 0],\n",
      "        [5, 8, 8, 2],\n",
      "        [0, 7, 8, 6]]) \n",
      "\n",
      "a[2:, :] tensor([[5, 8, 8, 2],\n",
      "        [0, 7, 8, 6]]) \n",
      "\n",
      "a[:, -1] tensor([4, 0, 2, 6])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randint(0, 10, (4, 4))\n",
    "print('a:', a, '\\n')\n",
    "\n",
    "# Slice using ranges\n",
    "print('a[2:, :]', a[2:, :], '\\n')\n",
    "\n",
    "# Can count backwards using negative indices\n",
    "print('a[:, -1]', a[:, -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resizing and reshaping tensors is also quite simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turn tensor into a 1 dimensional array:\n",
      "Before size: torch.Size([3, 3])\n",
      "tensor([[4, 2, 8],\n",
      "        [9, 0, 4],\n",
      "        [6, 5, 1]]) \n",
      "\n",
      "After size: torch.Size([1, 9])\n",
      "tensor([[4, 2, 8, 9, 0, 4, 6, 5, 1]])\n"
     ]
    }
   ],
   "source": [
    "print('Turn tensor into a 1 dimensional array:')\n",
    "a = torch.randint(0, 10, (3, 3))\n",
    "\n",
    "print(f'Before size: {a.size()}')\n",
    "print(a, '\\n')\n",
    "\n",
    "a = a.view(1, 9)\n",
    "print(f'After size: {a.size()}')\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing a `Tensor` to and from an `array` is also quite simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tensor from array\n",
    "arr = np.array([1,2])\n",
    "torch.from_numpy(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 2.], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tensor to array\n",
    "t = torch.Tensor([1, 2])\n",
    "t.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving `Tensor`s to the GPU is also quite simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.Tensor([1, 2]) # on CPU\n",
    "if torch.cuda.is_available():\n",
    "    t = t.cuda() # on GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Differentiation\n",
    "https://pytorch.org/tutorials/beginner/basics/autograd_tutorial.html\n",
    "\n",
    "Derivatives and gradients are critical to a large number of machine learning algorithms. One of the key benefits of PyTorch is that these can be computed automatically.\n",
    "\n",
    "We'll demonstrate this using the following example. Suppose we have some data $x$ and $y$, and want to fit a model:\n",
    "$$ \\hat{y} = mx + b $$\n",
    "by minimizing the loss function:\n",
    "$$ L(y, \\hat{y}) = \\frac{1}{2}(y - \\hat{y})^2 $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.5903, 2.2638, 2.9373, 3.6109], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "x = torch.tensor([1.,  2,  3,  4])  # requires_grad = False by default\n",
    "y = torch.tensor([0., -1, -2, -3])\n",
    "\n",
    "# Initialize parameters\n",
    "m = torch.rand(1, requires_grad=True)\n",
    "b = torch.rand(1, requires_grad=True)\n",
    "\n",
    "# Define regression function\n",
    "y_hat = m * x + b\n",
    "print(y_hat)\n",
    "# Define loss\n",
    "loss = torch.mean(0.5 * (y - y_hat)**2)\n",
    "\n",
    "loss.backward() # Backprop the gradients of the loss w.r.t other variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the $x$ and $y$ values, you can see that the perfect values for our parameters are $m$=-1 and $b$=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain the gradient of the $L$ w.r.t $m$ and $b$ you need only run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients:\n",
      "dL/dm: 12.3434\n",
      "dL/db: 4.1006\n"
     ]
    }
   ],
   "source": [
    "# Gradients\n",
    "print('Gradients:')\n",
    "print('dL/dm: %0.4f' % m.grad)\n",
    "print('dL/db: %0.4f' % b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Models\n",
    "\n",
    "While automatic differentiation is in itself a useful feature, it can be quite tedious to keep track of all of the different parameters and gradients for more complicated models. In order to make life simple, PyTorch defines a `torch.nn.Module` class which handles all of these details for you. To paraphrase the PyTorch documentation, this is the base class for all neural network modules, and whenever you define a model it should be a subclass of this class.\n",
    "\n",
    "There are two main functions you need to implement for a `Module` class:\n",
    "- $__init__$: Function first called when object is initialized. Used to set parameters, etc.\n",
    "- $__forward__$: When the model is called, this forwards the inputs through the model.\n",
    "\n",
    "Here is an example implementation of the simple linear model given above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3., 4.]) tensor([0.9696, 1.0945, 1.2193, 1.3442], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LinearModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"This method is called when you instantiate a new LinearModel object.\n",
    "        \n",
    "        You should use it to define the parameters/layers of your model.\n",
    "        \"\"\"\n",
    "        # Whenever you define a new nn.Module you should start the __init__()\n",
    "        # method with the following line. Remember to replace `LinearModel` \n",
    "        # with whatever you are calling your model.\n",
    "        super(LinearModel, self).__init__()\n",
    "        \n",
    "        # Now we define the parameters used by the model.\n",
    "        self.m = torch.nn.Parameter(torch.rand(1))\n",
    "        self.b = torch.nn.Parameter(torch.rand(1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"This method computes the output of the model.\n",
    "        \n",
    "        Args:\n",
    "            x: The input data.\n",
    "        \"\"\"\n",
    "        return self.m * x + self.b\n",
    "\n",
    "# Initialize model\n",
    "model = LinearModel()\n",
    "\n",
    "# Example forward pass. Note that we use model(x) not model.forward(x) !!! \n",
    "y_hat = model(x)\n",
    "print(x, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train this model we need to pick an optimizer such as SGD, AdaDelta, ADAM, etc. There are many options in `torch.optim`. When initializing an optimizer, the first argument will be the collection of variables you want optimized. To obtain a list of all of the trainable parameters of a model you can call the `nn.Module.parameters()` method. For example, the following code initalizes a SGD optimizer for the model defined above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training is done in a loop. The general structure is:\n",
    "\n",
    "1. Clear the gradients.\n",
    "2. Evaluate the model.\n",
    "3. Calculate the loss.\n",
    "4. Backpropagate.\n",
    "5. Perform an optimization step.\n",
    "6. (Once in a while) Print monitoring metrics.\n",
    "\n",
    "For example, we can train our linear model by running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 - Loss: 4.320335\n",
      "Iteration 1000 - Loss: 0.000000\n",
      "Iteration 2000 - Loss: 0.000000\n",
      "Iteration 3000 - Loss: 0.000000\n",
      "Iteration 4000 - Loss: 0.000000\n",
      "Iteration 5000 - Loss: 0.000000\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "for i in range(5001):\n",
    "    optimizer.zero_grad()\n",
    "    y_hat = model(x) # calling model() calls the forward function\n",
    "    loss = torch.mean(0.5 * (y - y_hat)**2)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i % 1000 == 0:\n",
    "        time.sleep(1) # DO NOT INCLUDE THIS IN YOUR CODE !!! Only for demo.\n",
    "        print(f'Iteration {i} - Loss: {loss.item():0.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that the final parameters are what we expect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-5.9605e-08, -1.0000e+00, -2.0000e+00, -3.0000e+00],\n",
      "       grad_fn=<AddBackward0>) tensor([ 0., -1., -2., -3.])\n"
     ]
    }
   ],
   "source": [
    "print(model(x), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final parameters:\n",
      "m: -1.00\n",
      "b: 1.00\n"
     ]
    }
   ],
   "source": [
    "print('Final parameters:')\n",
    "print('m: %0.2f' % model.m)\n",
    "print('b: %0.2f' % model.b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CASE STUDY: POS Tagging!\n",
    "\n",
    "Now let's dive into an example that is more relevant to NLP and is relevant to your HW3, part-of-speech tagging! We will be building up code up until the point where you will be able to process the POS data into tensors, then train a simple model on it.\n",
    "The code we are building up to forms the basis of the code in the homework assignment.\n",
    "\n",
    "To start, we'll need some data to train and evaluate on. First download the train and dev POS data `twitter_train.pos` and `twitter_dev.pos` into the same directory as this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First data point:\n",
      "\t @paulwalk\tX\tNone\tNone\n",
      "\t It\tPRON\tNone\tNone\n",
      "\t 's\tVERB\tNone\tNone\n",
      "\t the\tDET\tNone\tNone\n",
      "\t view\tNOUN\tNone\tNone\n",
      "\t from\tADP\tNone\tNone\n",
      "\t where\tADV\tNone\tNone\n",
      "\t I\tPRON\tNone\tNone\n",
      "\t 'm\tVERB\tNone\tNone\n",
      "\t living\tVERB\tNone\tNone\n",
      "\t for\tADP\tNone\tNone\n",
      "\t two\tNUM\tNone\tNone\n",
      "\t weeks\tNOUN\tNone\tNone\n",
      "\t .\t.\tNone\tNone\n",
      "\t Empire\tNOUN\tNone\tNone\n",
      "\t State\tNOUN\tNone\tNone\n",
      "\t Building\tNOUN\tNone\tNone\n",
      "\t =\tX\tNone\tNone\n",
      "\t ESB\tNOUN\tNone\tNone\n",
      "\t .\t.\tNone\tNone\n",
      "\t Pretty\tADV\tNone\tNone\n",
      "\t bad\tADJ\tNone\tNone\n",
      "\t storm\tNOUN\tNone\tNone\n",
      "\t here\tADV\tNone\tNone\n",
      "\t last\tADJ\tNone\tNone\n",
      "\t evening\tNOUN\tNone\tNone\n",
      "\t .\t.\tNone\tNone\n",
      "\t \n"
     ]
    }
   ],
   "source": [
    "print(\"First data point:\")\n",
    "with open('twitter_train.pos', 'r') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        print('\\t', line)\n",
    "        if line == '':\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now be introducing three new components which are vital to training (NLP) models:\n",
    "1. a `Vocabulary` object which converts from tokens/labels to integers. This part should also be able to handle padding so that batches can be easily created.\n",
    "2. a `Dataset` object which takes in the data file and produces data tensors\n",
    "3. a `DataLoader` object which takes data tensors from `Dataset` and batches them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Vocabulary`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to get our data into Python and in a form that is usable by PyTorch. For text data this typically entails building a `Vocabulary`  of all of the words, then mapping words to integers corresponding to their place in the sorted vocabulary. This can be done as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "code_folding": [
     6,
     23,
     28,
     31,
     37,
     41,
     49
    ]
   },
   "outputs": [],
   "source": [
    "class Vocabulary():\n",
    "    \"\"\" Object holding vocabulary and mappings\n",
    "    Args:\n",
    "        word_list: ``list`` A list of words. Words assumed to be unique.\n",
    "        add_unk_token: ``bool` Whether to create an token for unknown tokens.\n",
    "    \"\"\"\n",
    "    def __init__(self, word_list, add_unk_token=False):\n",
    "        # create special tokens for padding and unknown words\n",
    "        self.pad_token = '<pad>'\n",
    "        self.unk_token = '<unk>' if add_unk_token else None\n",
    "\n",
    "        self.special_tokens = [self.pad_token]\n",
    "        if self.unk_token:\n",
    "            self.special_tokens += [self.unk_token]\n",
    "\n",
    "        self.word_list = word_list\n",
    "        \n",
    "        # maps from the token ID to the token\n",
    "        self.id_to_token = self.word_list + self.special_tokens\n",
    "        # maps from the token to its token ID\n",
    "        self.token_to_id = {token: id for id, token in\n",
    "                            enumerate(self.id_to_token)}\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\" Returns size of vocabulary \"\"\"\n",
    "        return len(self.token_to_id)\n",
    "    \n",
    "    @property\n",
    "    def pad_token_id(self):\n",
    "        return self.map_token_to_id(self.pad_token)\n",
    "        \n",
    "    def map_token_to_id(self, token: str):\n",
    "        \"\"\" Maps a single token to its token ID \"\"\"\n",
    "        if token not in self.token_to_id:\n",
    "            token = self.unk_token\n",
    "        return self.token_to_id[token]\n",
    "\n",
    "    def map_id_to_token(self, id: int):\n",
    "        \"\"\" Maps a single token ID to its token \"\"\"\n",
    "        return self.id_to_token[id]\n",
    "\n",
    "    def map_tokens_to_ids(self, tokens: list, max_length: int = None):\n",
    "        \"\"\" Maps a list of tokens to a list of token IDs \"\"\"\n",
    "        # truncate extra tokens and pad to `max_length`\n",
    "        if max_length:\n",
    "            tokens = tokens[:max_length]\n",
    "            tokens = tokens + [self.pad_token]*(max_length-len(tokens))\n",
    "        return [self.map_token_to_id(token) for token in tokens]\n",
    "\n",
    "    def map_ids_to_tokens(self, ids: list, filter_padding=True):\n",
    "        \"\"\" Maps a list of token IDs to a list of token \"\"\"\n",
    "        tokens = [self.map_id_to_token(id) for id in ids]\n",
    "        if filter_padding:\n",
    "            tokens = [t for t in tokens if t != self.pad_token]\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a vocabulary with a small amount of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = ['i', 'like', 'dogs', '!']\n",
    "vocab = Vocabulary(word_list, add_unk_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "map from the token \"i\" to its token ID, then back again\n",
      "0\n",
      "i\n"
     ]
    }
   ],
   "source": [
    "print('map from the token \"i\" to its token ID, then back again')\n",
    "token_id = vocab.map_token_to_id('i')\n",
    "print(token_id)\n",
    "print(vocab.map_id_to_token(token_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what about a token not in our vocabulary like \"you\"?\n",
      "5\n",
      "<unk>\n"
     ]
    }
   ],
   "source": [
    "print('what about a token not in our vocabulary like \"you\"?')\n",
    "token_id = vocab.map_token_to_id('you')\n",
    "print(token_id)\n",
    "print(vocab.map_id_to_token(token_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping a sequence of tokens: '['i', 'like', 'dogs', '!']'\n",
      "[0, 1, 2, 3, 4, 4, 4, 4, 4, 4]\n",
      "['i', 'like', 'dogs', '!', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "token_ids = vocab.map_tokens_to_ids(['i', 'like', 'dogs', '!'], max_length=10)\n",
    "print(\"mapping a sequence of tokens: \\'['i', 'like', 'dogs', '!']\\'\")\n",
    "print(token_ids)\n",
    "print(vocab.map_ids_to_tokens(token_ids, filter_padding=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Dataset`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need a way to efficiently read in the data file and to process it into tensors. PyTorch provides an easy way to do this using the `torch.utils.data.Dataset` class. We will be creating our own class which inherits from this class. \n",
    "\n",
    "Helpful link: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A custom `Dataset` class must implement three functions: \n",
    "\n",
    "- $__init__$: The init functions is run once when instantisting the `Dataset` object.\n",
    "- $__len__$: The len function returns the number of data points in our dataset.\n",
    "- $__getitem__$. The getitem function returns a sample from the dataset give the index of the sample. The output of this part should be a dictionary of (mostly) PyTorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "code_folding": [
     1,
     22,
     25,
     36,
     45,
     50,
     55
    ]
   },
   "outputs": [],
   "source": [
    "class TwitterPOSDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_path, max_length=30):\n",
    "        self._max_length = max_length\n",
    "        self._dataset = []\n",
    "    \n",
    "        # read the dataset file, extracting tokens and tags\n",
    "        with open(data_path, 'r') as f:\n",
    "            tokens, tags = [], []\n",
    "            for line in f:\n",
    "                elements = line.strip().split('\\t')\n",
    "                # empty line means end of sentence\n",
    "                if elements == [\"\"]:\n",
    "                    self._dataset.append({'tokens': tokens, 'tags': tags})\n",
    "                    tokens, tags = [], []\n",
    "                else:\n",
    "                    tokens.append(elements[0].lower())\n",
    "                    tags.append(elements[1])\n",
    "        \n",
    "        # intiailize an empty vocabulary\n",
    "        self.token_vocab = None\n",
    "        self.tag_vocab = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._dataset)\n",
    "\n",
    "    def __getitem__(self, item: int):\n",
    "        # get the sample corresponding to the index\n",
    "        instance = self._dataset[item]\n",
    "        \n",
    "        # check the vocabulary has been set\n",
    "        assert self.token_vocab is not None\n",
    "        assert self.tag_vocab is not None\n",
    "        \n",
    "        # Convert inputs to tensors, then return\n",
    "        return self.tensorize(instance['tokens'], instance['tags'], self._max_length)\n",
    "    \n",
    "    def tensorize(self, tokens, tags=None, max_length=None):\n",
    "        # map the tokens and tags into their ID form\n",
    "        token_ids = self.token_vocab.map_tokens_to_ids(tokens, max_length)\n",
    "        tensor_dict = {'token_ids': torch.LongTensor(token_ids)}\n",
    "        if tags:\n",
    "            tag_ids = self.tag_vocab.map_tokens_to_ids(tags, max_length)\n",
    "            tensor_dict['tag_ids'] = torch.LongTensor(tag_ids)\n",
    "        return tensor_dict\n",
    "        \n",
    "    def get_tokens_list(self):\n",
    "        \"\"\" Returns set of tokens in dataset \"\"\"\n",
    "        tokens = [token for d in self._dataset for token in d['tokens']]\n",
    "        return sorted(set(tokens))\n",
    "\n",
    "    def get_tags_list(self):\n",
    "        \"\"\" Returns set of tags in dataset \"\"\"\n",
    "        tags = [tag for d in self._dataset for tag in d['tags']]\n",
    "        return sorted(set(tags))\n",
    "\n",
    "    def set_vocab(self, token_vocab: Vocabulary, tag_vocab: Vocabulary):\n",
    "        self.token_vocab = token_vocab\n",
    "        self.tag_vocab = tag_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create `Dataset` objects for our training and validation sets!\n",
    "A key step here is creating the `Vocabulary` for these datasets.\n",
    "We will use the list of words in the training set to intialize a `Vocabulary` object over the input words. \n",
    "We will also use list of tags to intialize a `Vocabulary` over the tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TwitterPOSDataset('twitter_train.pos')\n",
    "dev_dataset = TwitterPOSDataset('twitter_dev.pos')\n",
    "\n",
    "# Get list of tokens and tags seen in training set and use to create Vocabulary\n",
    "token_list = train_dataset.get_tokens_list()\n",
    "tag_list = train_dataset.get_tags_list()\n",
    "\n",
    "token_vocab = Vocabulary(token_list, add_unk_token=True)\n",
    "tag_vocab = Vocabulary(tag_list)\n",
    "\n",
    "# Update the train/dev set with vocabulary. Notice we created the vocabulary using the training set\n",
    "train_dataset.set_vocab(token_vocab, tag_vocab)\n",
    "dev_dataset.set_vocab(token_vocab, tag_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training set: 379\n",
      "Size of validation set: 112\n"
     ]
    }
   ],
   "source": [
    "print(f'Size of training set: {len(train_dataset)}')\n",
    "print(f'Size of validation set: {len(dev_dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print out one data point of the tensorized data and see what it looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token_ids': tensor([ 361, 1276, 2102, 2087,   90, 2267, 1276,   88, 1093,  576, 2095, 2370,\n",
      "        2370, 2370, 2370, 2370, 2370, 2370, 2370, 2370, 2370, 2370, 2370, 2370,\n",
      "        2370, 2370, 2370, 2370, 2370, 2370]), 'tag_ids': tensor([11,  8, 10,  5, 10,  3,  8, 10, 10, 10,  3, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12])}\n",
      "\n",
      "Tokens: ['@miss_soto', 'i', 'think', 'that', \"'s\", 'when', 'i', \"'m\", 'gonna', 'be', 'there']\n",
      "Tags:   ['X', 'PRON', 'VERB', 'DET', 'VERB', 'ADV', 'PRON', 'VERB', 'VERB', 'VERB', 'ADV']\n"
     ]
    }
   ],
   "source": [
    "instance = train_dataset[2]\n",
    "print(instance)\n",
    "\n",
    "tokens = train_dataset.token_vocab.map_ids_to_tokens(instance['token_ids'])\n",
    "tags = train_dataset.tag_vocab.map_ids_to_tokens(instance['tag_ids'])\n",
    "print()\n",
    "print(f'Tokens: {tokens}')\n",
    "print(f'Tags:   {tags}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `DataLoader`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point our data is in a tensor, and we can create context windows using only PyTorch operations.\n",
    "Now we need a way to generate batches of data for training and evaluation.\n",
    "To do this, we will wrap our `Dataset` objects in a `torch.utils.data.DataLoader` object, which will automatically batch datapoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting batch_size to be 3\n"
     ]
    }
   ],
   "source": [
    "batch_size = 3\n",
    "print(f'Setting batch_size to be {batch_size}')\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size)\n",
    "dev_dataloader = torch.utils.data.DataLoader(dev_dataset, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do one iteration over our training set to see what a batch looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token_ids': tensor([[ 376, 1318,   90, 2089, 2217, 1053, 2268, 1276,   88, 1431, 1033, 2175,\n",
      "         2259,  118,  916, 1997,  663,  228,  934,  118, 1748,  561, 2018, 1164,\n",
      "         1391,  943,  118, 2370, 2370, 2370],\n",
      "        [1939,  617, 2067, 2141,  162, 1394, 1022,  721, 2141, 1550, 1576,  128,\n",
      "         2089,  484,  817,  944, 1001,  499, 1258,   32, 2370, 2370, 2370, 2370,\n",
      "         2370, 2370, 2370, 2370, 2370, 2370],\n",
      "        [ 361, 1276, 2102, 2087,   90, 2267, 1276,   88, 1093,  576, 2095, 2370,\n",
      "         2370, 2370, 2370, 2370, 2370, 2370, 2370, 2370, 2370, 2370, 2370, 2370,\n",
      "         2370, 2370, 2370, 2370, 2370, 2370]]), 'tag_ids': tensor([[11,  8, 10,  5,  6,  2,  3,  8, 10, 10,  2,  7,  6,  0,  6,  6,  6, 11,\n",
      "          6,  0,  3,  1,  6,  3,  1,  6,  0, 12, 12, 12],\n",
      "        [ 1,  6,  6,  6,  7, 10,  7,  6,  6,  6,  6,  0,  5,  5,  6,  6, 10,  6,\n",
      "         11, 11, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12],\n",
      "        [11,  8, 10,  5, 10,  3,  8, 10, 10, 10,  3, 12, 12, 12, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12]])} \n",
      "\n",
      "Size of tag_ids: torch.Size([3, 30])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    print(batch, '\\n')\n",
    "    print(f'Size of tag_ids: {batch[\"tag_ids\"].size()}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Now that we can read in the data, it is time to build our model.\n",
    "We will build a very simple LSTM based tagger! Note that this is pretty similar to the code in `simple_tagger.py` in your homework, but with a lot of things hardcoded.\n",
    "\n",
    "Useful links:\n",
    "- Embedding Layer: https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html\n",
    "- LSTMs: https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n",
    "- Linear Layer: https://pytorch.org/docs/stable/generated/torch.nn.Linear.html?highlight=linear#torch.nn.Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "code_folding": [
     1,
     19
    ]
   },
   "outputs": [],
   "source": [
    "class SimpleTagger(torch.nn.Module):\n",
    "    def __init__(self, token_vocab, tag_vocab):\n",
    "        super(SimpleTagger, self).__init__()\n",
    "        self.token_vocab = token_vocab\n",
    "        self.tag_vocab = tag_vocab\n",
    "        self.num_tags = len(self.tag_vocab)\n",
    "        \n",
    "        # Initialize random embeddings of size 50 for each word in your token vocabulary\n",
    "        self._embeddings = torch.nn.Embedding(len(token_vocab), 50)\n",
    "        \n",
    "        # Initialize a single-layer bidirectional LSTM encoder\n",
    "        self._encoder = torch.nn.LSTM(input_size=50, hidden_size=25, num_layers=1, bidirectional=True)\n",
    "        \n",
    "        # _encoder a Linear layer which projects from the hidden state size to the number of tags\n",
    "        self._tag_projection = torch.nn.Linear(in_features=50, out_features=len(self.tag_vocab))\n",
    "\n",
    "        # Loss will be a Cross Entropy Loss over the tags (except the padding token)\n",
    "        self.loss = torch.nn.CrossEntropyLoss(ignore_index=self.tag_vocab.pad_token_id)\n",
    "\n",
    "    def forward(self, token_ids, tag_ids=None):\n",
    "        # Create mask over all the positions where the input is padded\n",
    "        mask = token_ids != self.token_vocab.pad_token_id\n",
    "        \n",
    "        # Embed Inputs\n",
    "        embeddings = self._embeddings(token_ids).permute(1, 0, 2)\n",
    "        # Feed embeddings through LSTM\n",
    "        encoder_outputs = self._encoder(embeddings)[0].permute(1, 0, 2)\n",
    "        # Project output of LSTM through linear layer to get logits\n",
    "        tag_logits = self._tag_projection(encoder_outputs)\n",
    "        # Get the maximum score for each position as the predicted tag\n",
    "        pred_tag_ids = torch.max(tag_logits, dim=-1)[1]\n",
    "\n",
    "        output_dict = {\n",
    "            'pred_tag_ids': pred_tag_ids,\n",
    "            'tag_logits': tag_logits,\n",
    "            'tag_probs': torch.nn.functional.softmax(tag_logits, dim=-1) # covert logits to probs\n",
    "        }\n",
    "        # Compute loss and accuracy if gold tags are provided\n",
    "        if tag_ids is not None:\n",
    "            loss = self.loss(tag_logits.view(-1, self.num_tags), tag_ids.view(-1))\n",
    "            output_dict['loss'] = loss\n",
    "            \n",
    "            correct = pred_tag_ids == tag_ids # 1's in positions where pred matches gold\n",
    "            correct *= mask # zero out positions where mask is zero\n",
    "            output_dict['accuracy'] = torch.sum(correct)/torch.sum(mask)\n",
    "\n",
    "        return output_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "The training script essentially follows the same pattern that we used for the linear model above. However we have also added an evaluation step, and code for saving model checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "code_folding": [
     26,
     45
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0\n",
      "Train loss 2.2184017755110848 accuracy 0.2823981046676636\n",
      "Dev loss 1.8892717074070657 accuracy 0.40769562125205994\n",
      "Best so far\n",
      "\n",
      "Epoch 1\n",
      "Train loss 1.6247831381719786 accuracy 0.5223523378372192\n",
      "Dev loss 1.407148138220821 accuracy 0.5866950750350952\n",
      "Best so far\n",
      "\n",
      "Epoch 2\n",
      "Train loss 1.2275152156095077 accuracy 0.6613854169845581\n",
      "Dev loss 1.155178641634328 accuracy 0.6573066711425781\n",
      "Best so far\n",
      "\n",
      "Epoch 3\n",
      "Train loss 0.9917180402926845 accuracy 0.7219552397727966\n",
      "Dev loss 1.0195192662732941 accuracy 0.6941012740135193\n",
      "Best so far\n",
      "\n",
      "Epoch 4\n",
      "Train loss 0.8328757446485333 accuracy 0.7617589235305786\n",
      "Dev loss 0.9338890106550285 accuracy 0.7289190292358398\n",
      "Best so far\n",
      "\n",
      "Epoch 5\n",
      "Train loss 0.7124635504858475 accuracy 0.7966110706329346\n",
      "Dev loss 0.8751283083111048 accuracy 0.7454296350479126\n",
      "Best so far\n",
      "\n",
      "Epoch 6\n",
      "Train loss 0.6149178453202613 accuracy 0.8256787657737732\n",
      "Dev loss 0.8340495005249977 accuracy 0.7545357942581177\n",
      "Best so far\n",
      "\n",
      "Epoch 7\n",
      "Train loss 0.5332277208016227 accuracy 0.8514186143875122\n",
      "Dev loss 0.805136653993811 accuracy 0.762856662273407\n",
      "Best so far\n",
      "\n",
      "Epoch 8\n",
      "Train loss 0.46312322693638563 accuracy 0.874297559261322\n",
      "Dev loss 0.7852282811488424 accuracy 0.767623782157898\n",
      "Best so far\n",
      "\n",
      "Epoch 9\n",
      "Train loss 0.40191608911139354 accuracy 0.8913050293922424\n",
      "Dev loss 0.7722398534949336 accuracy 0.7737156748771667\n",
      "Best so far\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "################################\n",
    "# Setup\n",
    "################################\n",
    "# Create model\n",
    "model = SimpleTagger(token_vocab=token_vocab, tag_vocab=tag_vocab)\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "\n",
    "# Initialize optimizer.\n",
    "# Note: The learning rate is an important hyperparameters to tune\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "################################\n",
    "# Training and Evaluation!\n",
    "################################\n",
    "num_epochs = 10\n",
    "best_dev_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print('\\nEpoch', epoch)\n",
    "    # Training loop\n",
    "    model.train() # THIS PART IS VERY IMPORTANT TO SET BEFORE TRAINING\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    for batch in train_dataloader:\n",
    "        batch_size = batch['token_ids'].size(0)\n",
    "        optimizer.zero_grad()\n",
    "        output_dict = model(**batch)\n",
    "        loss = output_dict['loss']\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()*batch_size\n",
    "        accuracy = output_dict['accuracy']\n",
    "        train_acc += accuracy*batch_size\n",
    "    train_loss /= len(train_dataset)\n",
    "    train_acc /= len(train_dataset)\n",
    "    print(f'Train loss {train_loss} accuracy {train_acc}')\n",
    "    \n",
    "    # Evaluation loop\n",
    "    model.eval() # THIS PART IS VERY IMPORTANT TO SET BEFORE EVALUATION\n",
    "    dev_loss = 0\n",
    "    dev_acc = 0\n",
    "    for batch in dev_dataloader:\n",
    "        batch_size = batch['token_ids'].size(0)\n",
    "        output_dict = model(**batch)\n",
    "        dev_loss += output_dict['loss'].item()*batch_size\n",
    "        dev_acc += output_dict['accuracy']*batch_size\n",
    "    dev_loss /= len(dev_dataset)\n",
    "    dev_acc /= len(dev_dataset)\n",
    "    print(f'Dev loss {dev_loss} accuracy {dev_acc}')\n",
    "    \n",
    "    # Save best model\n",
    "    if dev_loss < best_dev_loss:\n",
    "        print('Best so far')\n",
    "        torch.save(model, 'model.pt')\n",
    "        best_dev_loss = dev_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Trained Models\n",
    "\n",
    "Loading a pretrained model can be done easily. To learn more about saving/loading models see https://pytorch.org/tutorials/beginner/saving_loading_models.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed in your own sentences!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token_ids': tensor([[1276, 2236, 2128,  901,  449, 2371,  118]])}\n",
      "[8, 10, 9, 10, 5, 6, 0]\n",
      "['PRON', 'VERB', 'PRT', 'VERB', 'DET', 'NOUN', '.']\n"
     ]
    }
   ],
   "source": [
    "sentence = 'i want to eat a pizza .'.lower().split()\n",
    "\n",
    "# convert sentence to tensor dictionar\n",
    "tensor_dict = train_dataset.tensorize(sentence)\n",
    "\n",
    "# unsqueeze first dimesion so batch size is 1\n",
    "tensor_dict['token_ids'] = tensor_dict['token_ids'].unsqueeze(0)\n",
    "print(tensor_dict)\n",
    "\n",
    "# feed through model\n",
    "output_dict = model(**tensor_dict)\n",
    "\n",
    "# get predicted tag IDs\n",
    "pred_tag_ids = output_dict['pred_tag_ids'].squeeze().tolist()\n",
    "print(pred_tag_ids)\n",
    "\n",
    "# convert tag IDs to tag names\n",
    "print(model.tag_vocab.map_ids_to_tokens(pred_tag_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've now seen at a high level how to create neural networks for NLP.\n",
    "You've also now seen the components that go around a model (e.g. training loops, data processing).\n",
    "Setting up these componenents in a flexible way can be tricky for NLP, as there are many issues that you have to take care of like padding, different vocabularies, etc.\n",
    "For example, how would you build upon this code to load in pre-trained embeddings, or use character embeddings?\n",
    "\n",
    "That's why there exist many libraries that take care of these boilerplate components so that you can focus on modeling.\n",
    "One of these libraries is [allennlp](https://allennlp.org/), and if you have time, I encourage you to take a look at it. \n",
    "It builds upon PyTorch so everything you've learned here is applicable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
